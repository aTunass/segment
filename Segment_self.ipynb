{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1yD7ygRkpB6xIJ_Ii0yRADmWShrKxb9TF",
      "authorship_tag": "ABX9TyON3cyobsELLfKRnvd21wqe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aTunass/segment/blob/main/Segment_self.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlM-kqLzvEO4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMBIEXVWD5nE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77c5ef2-aed4-4340-83ac-8458d26d54d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.15.2+cu118)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation_models_pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.65.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (8.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.0.1+cu118)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n",
            "  Downloading munch-3.0.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0)\n",
            "Collecting huggingface-hub (from timm==0.9.2->segmentation_models_pytorch)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm==0.9.2->segmentation_models_pytorch)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=ac7bb353838c251ab6072cfadc508e48131274a3bb5f790d22dce0911d378df0\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=f22ab536f41b58ef564018918265214a78d655cd340804386fa1be6964ffa09a\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: safetensors, munch, huggingface-hub, timm, pretrainedmodels, efficientnet-pytorch, segmentation_models_pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 huggingface-hub-0.14.1 munch-3.0.0 pretrainedmodels-0.7.4 safetensors-0.3.1 segmentation_models_pytorch-0.3.3 timm-0.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.10.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.7.0.72)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (23.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install segmentation_models_pytorch\n",
        "!pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
        "!wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz"
      ],
      "metadata": {
        "id": "VuX0nxbyFK2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e4709c-da46-42b7-9f8a-996022f3dfef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-17 14:17:48--  https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz [following]\n",
            "--2023-06-17 14:17:49--  https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n",
            "Reusing existing connection to thor.robots.ox.ac.uk:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791918971 (755M) [application/octet-stream]\n",
            "Saving to: ‘images.tar.gz’\n",
            "\n",
            "images.tar.gz       100%[===================>] 755.23M  15.4MB/s    in 42s     \n",
            "\n",
            "2023-06-17 14:18:31 (17.9 MB/s) - ‘images.tar.gz’ saved [791918971/791918971]\n",
            "\n",
            "--2023-06-17 14:18:31--  https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz [following]\n",
            "--2023-06-17 14:18:32--  https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n",
            "Reusing existing connection to thor.robots.ox.ac.uk:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19173078 (18M) [application/octet-stream]\n",
            "Saving to: ‘annotations.tar.gz’\n",
            "\n",
            "annotations.tar.gz  100%[===================>]  18.28M  9.96MB/s    in 1.8s    \n",
            "\n",
            "2023-06-17 14:18:34 (9.96 MB/s) - ‘annotations.tar.gz’ saved [19173078/19173078]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#giải nén file\n",
        "!tar -xf annotations.tar.gz\n",
        "!tar -xf images.tar.gz"
      ],
      "metadata": {
        "id": "7jqg1-wJFfCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchmetrics\n",
        "from torchmetrics import Dice, JaccardIndex #(IOU???)\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2 # np.array -> torch.tensor\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "HSumMrooFfZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask=cv2.imread(\"/content/annotations/trimaps/Abyssinian_100.png\", cv2.IMREAD_GRAYSCALE)\n",
        "plt.imshow(mask)\n",
        "print(mask)\n",
        "print(mask.shape)\n",
        "print(np.unique(mask)) #những loại pixel"
      ],
      "metadata": {
        "id": "PDcE5Uh3IJr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/content/images/Abyssinian_100.jpg\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(image)\n",
        "print(image.shape)"
      ],
      "metadata": {
        "id": "8aVNvxCIJk5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path_list = []\n",
        "with open(\"/content/annotations/trainval.txt\") as file_in:\n",
        "    for line in file_in:\n",
        "      img_path_list.append(line.split(\" \")[0])\n",
        "print(img_path_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "6jALY0qPc3DQ",
        "outputId": "82afc831-659e-4961-ab89-22191400ceb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-00cdf3946eac>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_path_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/annotations/trainval.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mimg_path_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/annotations/trainval.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = os.path.join(\"/content\", \"images\", \"{}.jpg\".format(img_path_list[17]))\n",
        "print(image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35g2TrxEdVFX",
        "outputId": "a489d85b-0fbe-4740-f5c6-b7c276916aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/images/Abyssinian_116.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_path = os.path.join(\"/content\", \"annotations\", \"trimaps\", \"{}.png\".format(img_path_list[17]))\n",
        "print(mask_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2M-EBLGdoIt",
        "outputId": "49211627-a5ed-48fd-e08b-3bb6860b183d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/annotations/trimaps/Abyssinian_116.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class my_dataset(Dataset):\n",
        "  def __init__(self, root_dir, txt_file, transform=None): #transform: agumentation + norm ([0-255] -> [0-1]) + np.array->tensor\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.txt_file = txt_file\n",
        "        self.transform = transform\n",
        "        self.img_path_list = []\n",
        "        with open(self.txt_file) as file_in:\n",
        "          for line in file_in:\n",
        "            self.img_path_list.append(line.split(\" \")[0])\n",
        "  def __len__(self):\n",
        "      return len(self.img_path_list)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      image_path = os.path.join(self.root_dir, \"images\", \"{}.jpg\".format(self.img_path_list[idx]))\n",
        "      mask_path = os.path.join(self.root_dir, \"annotations\", \"trimaps\", \"{}.png\".format(self.img_path_list[idx]))\n",
        "      image = cv2.imread(image_path)\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "      # foreground -> 1 chuyển các pixel về 0 và 1 để dễ phân loại và thỏa mãn yêu cầu class của pytorch phải từ 0 đến\n",
        "      # background 2 -> 0\n",
        "      # 3 -> 1\n",
        "      mask[mask == 2] = 0\n",
        "      mask[mask == 3] = 1\n",
        "      if self.transform is not None:\n",
        "          transformed = self.transform(image=image, mask=mask)\n",
        "          transformed_image = transformed['image']\n",
        "          transformed_mask = transformed['mask']\n",
        "      return transformed_image, transformed_mask"
      ],
      "metadata": {
        "id": "TWvFIjt2LECO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "            # The normalize code -> t.sub_(m).div_(s)\n",
        "        return tensor\n",
        "\n",
        "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
      ],
      "metadata": {
        "id": "JBRJnln4W4Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "DN2FiCFGzX1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy fn\n",
        "def accuracy_function(preds, targets):\n",
        "    preds_flat = preds.flatten()\n",
        "    targets_flat = targets.flatten()\n",
        "    acc = torch.sum(preds_flat == targets_flat)\n",
        "    return acc/targets_flat.shape[0]"
      ],
      "metadata": {
        "id": "wlDyApi10ziY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainsize = 384\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(width=trainsize, height=trainsize),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Blur(),\n",
        "    A.Sharpen(),\n",
        "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, always_apply=False, p=0.5),\n",
        "    A.Cutout(num_holes=5, max_h_size=25, max_w_size=25, fill_value=0),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), #mean và std là của tập imageNet, vì dử dụng pretrained model trên tập này\n",
        "    ToTensorV2(), # đưa về tensor\n",
        "])\n",
        "test_transform =  A.Compose([\n",
        "    A.Resize(width=trainsize, height=trainsize),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),\n",
        "    ToTensorV2(), # numpy.array -> torch.tensor (B, 3, H, W)\n",
        "])"
      ],
      "metadata": {
        "id": "mQ1DbY-dSpRF",
        "outputId": "968d9800-1513-4142-b1b3-331b1f42642c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bf80d24a5560>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_transform = A.Compose([\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomBrightnessContrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = my_dataset(\"/content\", \"/content/annotations/trainval.txt\", train_transform)\n",
        "test_dataset = my_dataset(\"/content\", \"/content/annotations/test.txt\", test_transform)\n",
        "image, mask = test_dataset.__getitem__(17)\n",
        "print(image.shape, mask.shape)\n",
        "print(mask.unique())\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(unorm(image).permute(1, 2, 0)) #channel đang nằm đầu phải chuyển về cuối mới hiển thị ảnh được\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_RATPYZdOuvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##model Unet\n",
        "def Unet_block(in_channels, out_channels):\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "      nn.ReLU(),\n",
        "  )\n",
        "class Unet(nn.Module):\n",
        "  def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.downsample = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
        "        self.block_down1 = Unet_block(3,64)\n",
        "        self.block_down2 = Unet_block(64,128)\n",
        "        self.block_down3 = Unet_block(128,256)\n",
        "        self.block_down4 = Unet_block(256,512)\n",
        "        self.block_neck = Unet_block(512,1024)\n",
        "        self.block_up1 = Unet_block(1024+512,512) #upsample tu neck ra channel:512 + channel tu down4: 1024 qua concat\n",
        "        self.block_up2 = Unet_block(256+512,256)\n",
        "        self.block_up3 = Unet_block(128+256,128)\n",
        "        self.block_up4 = Unet_block(64+128,64)\n",
        "\n",
        "        self.block_conv = nn.Conv2d(64,self.n_classes, 1) #ket qua: tensor (batch_size, n_classes, H, W)\n",
        "\n",
        "  def forward(self, x):\n",
        "     x1 = self.block_down1(x)\n",
        "     x = self.downsample(x1)\n",
        "     x2 = self.block_down2(x)\n",
        "     x = self.downsample(x2)\n",
        "     x3 = self.block_down3(x)\n",
        "     x = self.downsample(x3)\n",
        "     x4 = self.block_down4(x)\n",
        "     x = self.downsample(x4)\n",
        "\n",
        "     x = self.block_neck(x)\n",
        "\n",
        "     x = torch.cat([x4, self.upsample(x)], dim=1)# concat theo chieu channel\n",
        "     x = self.block_up1(x)\n",
        "     x = torch.cat([x3, self.upsample(x)], dim=1)\n",
        "     x = self.block_up2(x)\n",
        "     x = torch.cat([x2, self.upsample(x)], dim=1)\n",
        "     x = self.block_up3(x)\n",
        "     x = torch.cat([x1, self.upsample(x)], dim=1)\n",
        "     x = self.block_up4(x)\n",
        "\n",
        "     x = self.block_conv(x)\n",
        "     return x\n",
        "# model = Unet(1)\n",
        "# x = torch.rand(4,3, trainsize, trainsize)\n",
        "# print(x.shape)\n",
        "# y = model(x)\n",
        "# print(y.shape)\n",
        "# print(y.squeeze().shape)"
      ],
      "metadata": {
        "id": "Yz_fKTqQjLzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "TZfGkXZpyJGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "train_dataset = my_dataset(\"/content\", \"/content/annotations/trainval.txt\", train_transform)\n",
        "test_dataset = my_dataset(\"/content\", \"/content/annotations/test.txt\", test_transform)\n",
        "from torch.utils.data import DataLoader\n",
        "#load data\n",
        "batch_size = 8\n",
        "n_worker = os.cpu_count()\n",
        "print(\"num_worker\", n_worker)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=n_worker)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=n_worker)\n",
        "\n",
        "#model\n",
        "model = Unet(1).to(device)\n",
        "#loss_training\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "n_eps = 10\n",
        "#metrics tính toán độ chính xác\n",
        "dice_fn = torchmetrics.Dice(num_classes=2, average=\"macro\").to(device)\n",
        "iou_fn = torchmetrics.JaccardIndex(num_classes=2, task=\"binary\", average=\"macro\").to(device)\n",
        "#meter\n",
        "acc_meter = AverageMeter()\n",
        "train_loss_meter = AverageMeter()\n",
        "dice_meter = AverageMeter()\n",
        "iou_meter = AverageMeter()"
      ],
      "metadata": {
        "id": "CXELD3IMvumg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/UNet/model_ep_17.pth\", map_location=device))\n",
        "for ep in range(1,1+n_eps):\n",
        "  acc_meter.reset()\n",
        "  train_loss_meter.reset()\n",
        "  dice_meter.reset()\n",
        "  iou_meter.reset()\n",
        "  model.train() #chuyển sang chế độ training\n",
        "\n",
        "  for batch_id, (x,y) in enumerate(tqdm(train_dataloader), start=1):\n",
        "    optimizer.zero_grad()\n",
        "    n = x.shape[0] #bacth_size\n",
        "    x = x.to(device).float() #image\n",
        "    y = y.to(device).float() #mask\n",
        "    y_hat = model(x) #predict # -> logit (-vc, +vc) giá trị từ âm vc đến dương vc\n",
        "    # y_true: (batch_size ,384, 384), y_hat: (batch_size, n_classses ,384, 384), chuyển h_hat về cùng dạng y_true\n",
        "    y_hat = y_hat.squeeze()\n",
        "    loss = criterion(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      y_hat_mask = y_hat.sigmoid().round().long() # -> mask (0, 1)\n",
        "      dice_score = dice_fn(y_hat_mask, y.long())\n",
        "      iou_score = iou_fn(y_hat_mask, y.long())\n",
        "      accuracy = accuracy_function(y_hat_mask, y.long())\n",
        "      train_loss_meter.update(loss.item(), n)\n",
        "      iou_meter.update(iou_score.item(), n)\n",
        "      dice_meter.update(dice_score.item(), n)\n",
        "      acc_meter.update(accuracy.item(), n)\n",
        "  print(\"EP {}, train loss = {}, accuracy = {}, IoU = {}, dice = {}\".format(\n",
        "        ep, train_loss_meter.avg, acc_meter.avg, iou_meter.avg, dice_meter.avg\n",
        "    ))\n",
        "  if ep >= 5:\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/UNet/model_ep_{}.pth\".format(ep))\n"
      ],
      "metadata": {
        "id": "HAiQP0fe1w6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# đưa ra kết quả trên tập dữ liệu test\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/UNet/model_ep_17.pth\", map_location=device))\n",
        "model.eval()\n",
        "test_dice_meter = AverageMeter()\n",
        "test_iou_meter = AverageMeter()\n",
        "with torch.no_grad():\n",
        "  for batch_id, (x, y) in enumerate(tqdm(test_dataloader), start=1):\n",
        "        n = x.shape[0]\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device).float()\n",
        "        y_hat = model(x) #giá trị logit chưa qua sigmoid\n",
        "        y_hat = y_hat.squeeze()\n",
        "        y_hat_mask = y_hat.sigmoid().round().long()\n",
        "        dice_score = dice_fn(y_hat_mask, y.long())\n",
        "        iou_score = iou_fn(y_hat_mask, y.long())\n",
        "        test_dice_meter.update(dice_score.item(), n)\n",
        "        test_iou_meter.update(iou_score.item(), n)\n",
        "print(\"TEST: IoU = {}, dice = {}\".format(test_iou_meter.avg, test_dice_meter.avg))"
      ],
      "metadata": {
        "id": "WKNMfWCX2OVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/UNet/model_ep_10.pth\", map_location=device))\n",
        "model.eval()\n",
        "idx = random.randint(0, 100)\n",
        "with torch.no_grad():\n",
        "    x, y = test_dataset[20]\n",
        "    # print(x.shape, y.shape) (C, H, W) -> (1, C, H, W) -> model\n",
        "    x = x.to(device).float().unsqueeze(0)\n",
        "    y_hat = model(x).squeeze() #(1, 1, H, W) -> (H, W)\n",
        "    y_hat_mask = y_hat.sigmoid().round().long()\n",
        "    # x, y, y_hat_mask\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(unorm(x.squeeze().cpu()).permute(1, 2, 0)) # x (GPU) -> x(CPU)\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(y)\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(y_hat_mask.cpu())"
      ],
      "metadata": {
        "id": "bRCzkxJB5AoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_file = \"/content/annotations/trainval.txt\"\n",
        "my_list = []\n",
        "with open(txt_file) as file_in:\n",
        "            for line in file_in:\n",
        "                my_list.append(line.split(\" \")[0])\n",
        "print(my_list)"
      ],
      "metadata": {
        "id": "2FijbNbxMgSR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}